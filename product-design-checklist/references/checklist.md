# Product Design Cognitive Bias Checklist

Based on "Thinking, Fast and Slow" by Daniel Kahneman

## Design Output Check (Aligning with User Cognition - System 1)

### 1. Anchoring Effect
- [ ] Have I set appropriate price/feature anchors for comparison?
- [ ] Are default options strategically chosen?
- [ ] Do I use initial impressions to guide user expectations?
- [ ] Are comparison points clear and helpful?

**Design Applications**:
- Three-tier pricing (middle tier as anchor)
- Feature comparison tables
- Default selections that guide behavior
- Progress indicators (anchoring expectations)

### 2. Loss Aversion
- [ ] Am I framing benefits as "avoiding loss" rather than "gaining benefits"?
- [ ] Have I emphasized what users might lose by not using the product?
- [ ] Are risk-free trials or guarantees provided?
- [ ] Is the value proposition loss-framed when appropriate?

**Design Applications**:
- "Don't miss out" messaging
- "Avoid costly mistakes" positioning
- Free trial with easy cancellation
- Highlighting risks of not using the product

### 3. Peak-End Rule
- [ ] Have I designed memorable peak moments in the user journey?
- [ ] Does the experience end on a positive note?
- [ ] Are there moments of delight or surprise?
- [ ] Is the final interaction satisfying?

**Design Applications**:
- Welcome animations or celebrations
- Achievement moments
- Smooth, satisfying completion flows
- Thank you pages or confirmation screens

### 4. Cognitive Ease
- [ ] Is the design clear and easy to understand?
- [ ] Is branding consistent throughout?
- [ ] Does the design create positive emotions?
- [ ] Is information presented simply and clearly?

**Design Applications**:
- Clean, uncluttered interfaces
- Consistent visual language
- Pleasant color schemes
- Simple, clear copy

### 5. Framing Effect
- [ ] Am I using positive frames (success rate vs failure rate)?
- [ ] Are benefits framed in the most favorable way?
- [ ] Would reframing change user perception?
- [ ] Have I tested different frames?

**Design Applications**:
- "90% success rate" vs "10% failure rate"
- "Save time" vs "Don't waste time"
- "Join thousands" vs "Don't be left out"
- A/B test different framings

### 6. Mental Accounting
- [ ] Have I created appropriate mental categories for users?
- [ ] Are pricing/payment structures aligned with user mental models?
- [ ] Do I understand how users categorize this product/service?
- [ ] Can I leverage mental accounting for better UX?

**Design Applications**:
- "Reward yourself" vs "Investment" categories
- Subscription vs one-time payment framing
- Bundling products into logical categories
- Separate accounts for different purposes

### 7. Endowment Effect
- [ ] Do users value features more once they have them?
- [ ] Have I designed for user ownership and investment?
- [ ] Can I leverage users' attachment to their data/content?
- [ ] Are customization options that create ownership provided?

**Design Applications**:
- Personalized dashboards
- User-generated content
- Customizable interfaces
- Progress tracking and history

## Design Process Check (Avoiding Designer Biases - System 2)

### 8. Planning Fallacy
- [ ] Have I referenced similar projects' historical data?
- [ ] Have I consulted experienced designers/PMs?
- [ ] Have I broken down the project and estimated each part?
- [ ] Have I added buffer time for unexpected issues?

**Action**: Reference class forecasting, external perspective, task decomposition

### 9. Overconfidence
- [ ] Am I overestimating user adoption or engagement?
- [ ] Have I used external benchmarks (industry standards, competitor data)?
- [ ] Have I considered the possibility that users won't like it?
- [ ] Have I done a premortem (imagining the feature fails)?

**Action**: External benchmarks, premortem analysis, humility in predictions

### 10. Confirmation Bias
- [ ] Am I only looking for evidence that supports my design?
- [ ] Have I actively sought disconfirming evidence?
- [ ] Have I tested with users who might not like it?
- [ ] Am I ignoring negative feedback?

**Action**: Actively seek counterexamples, test with diverse users, listen to criticism

### 11. Regression to Mean
- [ ] Am I attributing user behavior changes to my design (when it might be regression)?
- [ ] Have I used statistical thinking, not just causal thinking?
- [ ] Am I distinguishing signal from noise in user data?
- [ ] Have I considered that extreme results often regress to average?

**Action**: Statistical analysis, distinguish signal from noise, avoid causal attribution

### 12. Availability Bias
- [ ] Am I letting recent user feedback dominate my design decisions?
- [ ] Have I considered historical feedback and data?
- [ ] Am I over-weighting vivid, recent examples?
- [ ] Have I built a feedback database to avoid recency bias?

**Action**: Build feedback database, review historical data, avoid recency bias

### 13. Anchoring in Design Process
- [ ] Am I anchored to initial design ideas or first solutions?
- [ ] Have I explored multiple design directions?
- [ ] Am I stuck on a particular approach?
- [ ] Have I cleared my assumptions and started fresh?

**Action**: Explore multiple directions, clear assumptions, delay commitment

### 14. Base Rate Neglect
- [ ] Am I ignoring industry benchmarks or statistical data?
- [ ] Have I considered general user behavior patterns before specific user stories?
- [ ] Am I using causal narratives instead of probability thinking?
- [ ] Have I started with base rates, then adjusted for specifics?

**Action**: Start with industry benchmarks, use statistical thinking, then adjust for specifics

## User Research Check (Interpreting Feedback Correctly)

### 15. Two Selves Distinction
- [ ] Am I measuring experience self (real-time usage) or memory self (retrospective feedback)?
- [ ] Have I tracked both what users say and what they do?
- [ ] Is there a conflict between user feedback and usage data?
- [ ] Have I designed for both experience and memory?

**Action**: Measure both real-time behavior and retrospective feedback, reconcile conflicts

### 16. Peak-End Rule in Research
- [ ] Are users remembering peaks and endings, not the full experience?
- [ ] Have I asked about specific moments vs overall experience?
- [ ] Am I over-weighting memorable moments in feedback?
- [ ] Have I measured the full journey, not just highlights?

**Action**: Ask about specific moments, measure full journey, don't over-weight peaks

### 17. Framing Effect in Questions
- [ ] How I frame questions affects user answers?
- [ ] Have I tested different question framings?
- [ ] Am I leading users with my questions?
- [ ] Are questions neutral and unbiased?

**Action**: Use neutral language, test different framings, avoid leading questions

### 18. Anchoring in Research
- [ ] Are initial questions or examples anchoring user responses?
- [ ] Have I randomized question order?
- [ ] Am I aware of how I'm setting expectations?
- [ ] Have I tested without anchors?

**Action**: Randomize order, be aware of anchors, test without context

### 19. Availability Bias in Research
- [ ] Am I only talking to engaged, vocal users?
- [ ] Have I reached out to silent users or churned users?
- [ ] Is my sample representative of all users?
- [ ] Am I over-weighting recent or vivid feedback?

**Action**: Diverse sampling, reach silent users, avoid recency bias

### 20. Confirmation Bias in Research
- [ ] Am I only asking questions that confirm my design?
- [ ] Have I asked about problems and dislikes?
- [ ] Am I ignoring negative feedback?
- [ ] Have I actively sought disconfirming evidence?

**Action**: Ask about problems, listen to criticism, seek disconfirming evidence

## Design Quality Indicators

A high-quality design process should have:

- âœ… User cognition principles applied (System 1 alignment)
- âœ… Designer biases checked (System 2 in process)
- âœ… External benchmarks referenced
- âœ… Diverse user feedback collected
- âœ… Statistical thinking applied
- âœ… Both experience and memory measured
- âœ… Multiple design directions explored
- âœ… Premortem analysis conducted

## Red Flags (Warning Signs)

Stop and reconsider if you notice:

- ðŸš© Users say they love it but don't use it (two selves conflict)
- ðŸš© You're only seeing positive feedback (confirmation bias)
- ðŸš© Recent feedback is dominating decisions (availability bias)
- ðŸš© You're ignoring industry benchmarks (base rate neglect)
- ðŸš© You're overconfident about user adoption (overconfidence)
- ðŸš© You haven't done a premortem (planning fallacy)
- ðŸš© You're anchored to initial design ideas (anchoring)

























































