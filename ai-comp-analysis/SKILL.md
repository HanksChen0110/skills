---
name: ai-comp-analysis
description: 当用户说「做AI产品竞品分析」时，使用AI产品冰山分析模型深度拆解竞品核心竞争力。通过能力边界测试、工作流反推、数据策略分析，从静态构成到动态流程全面评估AI产品。适用场景：产品规划、技术选型、市场研究、功能设计、投资决策。When user says "做AI产品竞品分析" or similar expressions, use AI Product Iceberg Model to deeply analyze competitor core capabilities. Through capability boundary testing, workflow reverse-engineering, and data strategy analysis, comprehensively evaluate AI products from static composition to dynamic processes. Use cases: product planning, technical selection, market research, feature design, investment decisions.
---

# AI 产品竞品分析

## 概述

本技能提供一套超越"功能清单"的AI竞品分析框架，基于**AI产品冰山分析模型**，从静态构成到动态流程深度拆解AI产品的核心竞争力。框架融合了传统5UX要素、AI原生XYZW模型，以及模型能力边界测试、工作流反推、认知偏差检查等系统化方法。

## 核心分析框架：AI产品冰山模型

### 冰山模型核心理念
- **水面之上**：用户可直接感知的部分（UI界面、功能按钮、交互流程）
- **水面之下**：决定产品核心体验的隐藏部分（模型、数据、AI人格、工作流等）
- **核心思想**：分析重点必须深入水下，通过压力测试和工作流反推揭示竞品真实能力

## 核心分析流程

### 阶段 0：分析前准备（关键基础）

在开始分析前，必须明确：

#### 0.1 分析目标与边界
- **分析目标用户**：产品经理/技术团队/投资人/其他？不同目标决定分析深度和重点
- **分析深度**：快速扫描（2-4小时）vs 深度拆解（1-3天）
- **分析时间范围**：一次性分析 vs 持续跟踪

#### 0.2 时间预算规划
根据可用时间选择分析深度：
- **极速版（1-2小时）**：仅执行快速诊断模式，聚焦核心维度
- **标准版（半天-1天）**：完成阶段1-3（数据采集、水面分析、核心能力拆解）
- **深度版（2-3天）**：完成全部8个阶段，产出完整分析报告

#### 0.3 分析优先级框架
- **核心维度（必须深入）**：模型能力边界、内部工作流、数据策略
- **重要维度（重点关注）**：AI人格、交互智能度、商业价值
- **辅助维度（快速扫描）**：UI设计、功能清单、市场定位

#### 0.4 建立分析假设清单
- **需要验证的核心假设**：如"竞品使用了RAG技术"、"竞品采用多Agent架构"
- **需要证伪的假设**：如"竞品只是简单调用API"、"竞品没有数据飞轮"
- **开放性问题**：如"竞品的差异化策略是什么？"、"竞品的护城河在哪里？"

#### 0.5 引入外部基准
- **行业标准测试集**：MMLU、HellaSwag、GSM8K等公开评测基准
- **第三方评测数据**：学术论文、行业报告中的模型评测结果
- **历史对比数据**：竞品在不同时期的性能变化

### 阶段 1：数据采集（带提示词模板）

#### 1.1 推荐工具
- **搜索工具**：
  - Kimi 深度搜索（适合技术深度挖掘）
  - Perplexity（适合实时信息检索）
  - Google（适合官方文档和新闻）
  - 官方文档/博客（一手信息源）
- **测试工具**：
  - 竞品产品本身（直接体验测试）
  - 浏览器开发者工具（分析网络请求）
  - 截图工具（记录测试过程）
- **可视化工具**：
  - Excel/Google Sheets（绘制对比矩阵）
  - 在线雷达图工具（如：Chart.js、ECharts）
  - 思维导图工具（整理分析框架）

#### 1.2 信息检索方法
使用 AI 搜索工具或官方文档进行一手信息检索。
- **推荐操作**：使用专属提示词模板以获取深度信息。详细模板请参阅 [prompts.md](references/prompts.md)

#### 1.3 异常情况处理
如果遇到以下情况，采用对应策略：
- **竞品信息不足或无法获取**：
  - 策略1：通过公开渠道（官网、博客、招聘信息）推断
  - 策略2：通过用户反馈和第三方评测补充
  - 策略3：明确标注"信息不足"，降低相关推测的置信度
- **竞品是新产品，没有公开信息**：
  - 策略1：重点分析产品界面和交互设计
  - 策略2：通过压力测试直接评估能力边界
  - 策略3：关注产品定位和差异化策略
- **竞品需要付费才能体验**：
  - 策略1：使用免费试用期进行测试
  - 策略2：通过公开演示视频和用户评价分析
  - 策略3：关注定价策略和商业模式
- **竞品在不同地区有不同版本**：
  - 策略1：明确标注分析的版本和地区
  - 策略2：对比不同版本的差异
  - 策略3：分析版本差异背后的策略考量

### 阶段 2：水面之上分析（传统5UX框架）

按照五要素层级进行基础分析：

- **战略层**：分析 **XYZ 定位**（收益化、垂直化、场景化）及**技术定位矩阵**（赋能/替代 × 感知/认知）
- **范围层**：分析**应用场景（X/Y/Z 轴）**的具体实现与功能集，特别关注**防伪节点**
- **结构层**：重点关注 **W 轴（数据闭环）**，即反馈机制与进化逻辑
- **框架层**：分析**人机协作界面**与交互框架
- **表现层**：评估 **AIGC 产出质量**与框架效应

**水面-水下连接点分析**：
- 为什么选择这种交互方式？（背后反映的技术决策）
- 为什么是这个入口/触发时机？（反映的产品策略）
- UI设计选择背后的技术约束（如：为什么限制输入长度？可能反映模型上下文窗口限制）

### 阶段 3：水面之下分析（核心能力拆解）

#### 3.1 静态构成分析：模型能力边界与压力测试

**压力测试用例设计**：设计一套"压力测试用例"来评估模型的"智商"

**测试维度**：
- **逻辑推理能力**：逻辑悖论问题、复杂推理链
- **知识边界与时效性**：上周发生的具体新闻、最新数据
- **多轮上下文记忆能力**：连续对话10轮后的回溯提问
- **安全与伦理护栏**：诱导性、边缘性问题
- **抗干扰与纠错能力**：故意输入错别字或模糊指令
- **边缘场景测试**：
  - 多语言混合输入（中英文混杂、特殊字符）
  - 极端上下文长度（超长输入、超短输入）
  - 不同环境下的表现差异（移动端vs桌面端、不同网络环境）

**量化评估体系**：

**评分标准（1-10分）**：
- **9-10分（行业领先）**：明显超越行业平均水平，在特定维度达到顶尖水平
  - 示例：逻辑推理能力9分 = 能解决复杂逻辑悖论，推理链清晰完整
- **7-8分（行业平均以上）**：优于行业平均水平，但在某些细节上仍有改进空间
  - 示例：知识时效性7分 = 能回答大部分最新信息，但偶尔有延迟
- **5-6分（行业平均）**：达到行业基本要求，无明显短板也无突出优势
  - 示例：上下文记忆6分 = 能记住5-10轮对话，但更长对话会丢失信息
- **3-4分（低于行业平均）**：存在明显不足，影响用户体验
  - 示例：安全护栏3分 = 容易被诱导，缺乏有效的安全机制
- **1-2分（明显不足）**：严重缺陷，基本无法满足需求
  - 示例：抗干扰能力2分 = 无法处理错别字，完全无法理解模糊指令

**评分记录方法**：
- 为每个测试维度设计3-5个具体测试用例
- 记录测试结果和评分依据
- 标注测试时间和测试环境
- 示例记录格式：
  ```
  逻辑推理能力：8/10分
  - 测试用例1：逻辑悖论问题 → 正确识别并解释（+2分）
  - 测试用例2：复杂推理链 → 推理过程清晰（+2分）
  - 测试用例3：多步骤问题 → 部分步骤有遗漏（-1分）
  - 测试用例4：条件推理 → 处理准确（+2分）
  - 测试用例5：类比推理 → 表现良好（+2分）
  ```

**能力边界可视化**：
- 绘制"能力雷达图"，直观展示竞品在各维度的表现
- 建议维度：逻辑推理、知识时效性、上下文记忆、安全护栏、抗干扰能力、交互智能度
- 可同时绘制多个竞品的雷达图进行对比

**对比基准**：
- 与行业平均水平对比（如：行业平均逻辑推理能力为6分）
- 与头部产品对比（如：GPT-4在逻辑推理方面为9分）
- 与自身历史版本对比（如：v2.0相比v1.0提升了2分）

#### 3.2 交互智能度评估

评估模型的"情商"，即它如何与用户沟通：
- **澄清与追问能力**：面对模糊指令时，是直接猜测还是会主动追问澄清，二次确认
- **优雅的失败案例（Graceful Failure）**：当遇到无法解决的问题时，是生硬拒绝还是提供建设性方案

#### 3.3 AI Persona（AI人格/人设）定义

分析AI产品如何塑造其独特的"人格"以建立用户情感链接：
- **角色定位**：专家、伙伴、仆人，还是创意家？
- **语言风格**：措辞、语调、网感词汇、Emoji的使用
- **交互细节**：响应速度、"思考中"的状态呈现等

#### 3.4 数据策略（Data Strategy）推断

通过产品的公开信息和交互设计，反推其可能的数据飞轮策略：
- **用户反馈机制**：点赞/点踩、反馈按钮
- **数据激励体系**：积分、徽章、更高权限等
- **核心数据壁垒**：是UGC内容，还是私有化数据？

### 阶段 4：内部工作流反推（动态分析流程）

**核心任务**：通过测试与观察，绘制出竞品内部可能的"思考链条"

**反推工作流的四步法**：

1. **意图识别层（Intent Recognition）**
   - 判断任务：判断产品如何对用户的输入进行分类和定性
   - 测试方法：分别输入"闲聊型"、"问答型"、"指令型"的指令，观察产品的初步反应和处理路径有何不同

2. **决策与规划层（Decision & Planning）**
   - 判断任务：判断产品何时、以及如何决定调用外部"工具"（Tools）
   - 测试方法：提出必须依赖外部信息（如实时天气、最新股价）或特定计算（如复杂数学题）才能解决的问题，观察其是否能成功调用相应工具

3. **知识检索层（Knowledge Retrieval/RAG）**
   - 判断任务：判断产品是否应用了RAG（检索增强生成）技术来调用私有知识库
   - 测试方法：提出只有该产品特定领域（如某款游戏的攻略、某公司的财报）才可能知道的精准问题。观察其回答是否超出通用大模型的知识范围，以及是否提供"来源"或"引用链接"

4. **生成与封装层（Generation & Packaging）**
   - 判断任务：判断产品的最终输出是否经过了后处理
   - 测试方法：观察最终内容的格式、排版、是否有安全审查机制（例如对敏感词的替换）、是否固定的免责声明等

**推测置信度标记系统**：
- **高度确信（90%+）**：通过多次测试验证，有明确证据支持
- **中等确信（50-90%）**：有部分证据，但存在其他可能性
- **低确信（<50%）**：基于有限观察的推测，需要更多验证
- 标记方法：在分析报告中用[高/中/低]标记每个推测的置信度

**反证测试**：
- 主动寻找不符合预期的证据（如：如果竞品表现不符合推测，如何解释？）
- 设置"竞品失败案例"分析维度（哪些场景下竞品表现差？为什么？）
- 考虑"竞品可能故意隐藏的能力"（某些能力可能被限制而非不存在）

### 阶段 5：商业与用户视角分析

#### 5.1 商业模式分析
- **盈利模式**：如何盈利？（订阅制、按量付费、免费+增值服务）
- **定价策略**：价格定位如何？与竞品对比
- **市场定位**：目标用户是谁？市场细分策略
- **竞争策略**：差异化策略是什么？护城河在哪里？

#### 5.2 用户价值感知分析
- **用户真正关心什么**：不是技术，而是体验和结果
- **用户痛点**：竞品解决了用户的哪些核心痛点？
- **用户旅程**：用户如何使用竞品？关键触点在哪里？
- **价值主张**：竞品向用户传达的核心价值是什么？

#### 5.3 市场定位与竞争格局
- **目标用户画像**：竞品瞄准哪类用户？
- **市场位置**：在市场上的相对位置（领导者、挑战者、跟随者）
- **竞争优势**：相比其他竞品的核心优势
- **竞争劣势**：可能存在的薄弱环节

### 阶段 6：多竞品横向对比（如适用）

#### 6.1 对比矩阵设计
- 选择3-5个核心竞品进行对比
- 建立统一的评估维度（模型能力、交互智能度、AI人格、数据策略、商业价值等）
- 每个维度进行量化评分（1-10分）
- 绘制对比雷达图，直观展示各竞品的优劣势

#### 6.2 差异化分析
- **核心差异**：各竞品之间的核心差异是什么？
- **差异化策略**：每个竞品选择了什么差异化路径？
- **差异化效果**：差异化是否成功？用户是否买单？

#### 6.3 竞争格局分析
- **市场分层**：竞品在市场上的相对位置
- **竞争态势**：是直接竞争还是差异化竞争？
- **趋势判断**：哪些竞品在上升？哪些在下降？为什么？

### 阶段 7：认知偏差检查与质量控制

#### 7.1 分析过程中的认知偏差防范
- **锚定效应检查**：是否被"冰山模型"框架过度锚定？是否忽略了水面之上的价值？
- **可得性偏差检查**：是否只考虑了容易想到的测试用例？是否忽略了罕见但关键的边缘场景？
- **确认偏误检查**：是否只寻找支持"冰山模型"的证据？是否忽略了反例和不符合预期的表现？
- **过度自信检查**：是否高估了通过测试就能完全理解竞品的能力？是否忽略了推测的不确定性？

#### 7.2 分析质量保障机制
- **验证机制**：
  - 推测验证方法：通过公开信息验证（技术博客、论文、招聘信息）、通过多次测试验证、通过第三方信息交叉验证
  - 外部基准对比：引入行业标准测试集和第三方评测数据
  - 专家评审：邀请技术专家或产品专家评审分析报告
- **更新机制**：
  - 分析报告有效期：明确标注分析报告的有效期（建议3-6个月）
  - 定期更新机制：设置定期更新计划（月度/季度）
  - 触发更新条件：竞品发布重大更新、发现分析报告中的错误、市场环境发生重大变化
- **标准化流程**：
  - 分析模板：建立不同行业、不同产品类型的分析模板
  - 知识库积累：积累测试用例、发现的模式、常见陷阱
  - 质量评估标准：多维度覆盖（技术+商业+用户）、有量化评估和对比、有置信度标记和验证机制、有明确的结论和行动建议

### 阶段 8：综合评估与报告输出

#### 8.1 SWOT 分析
- **优势（Strengths）**：技术壁垒、数据积淀、特定场景深度
- **劣势（Weaknesses）**：响应时延、成本控制、badcase 覆盖
- **机会（Opportunities）**：未被满足的垂直需求、长尾场景
- **威胁（Threats）**：大厂通用模型降维打击、政策合规性

#### 8.2 分析报告结构（参考模板）

- **执行摘要**（1-2页）：核心发现、关键结论、行动建议
- **分析目标与范围**（1页）：分析目标、分析范围、分析时间
- **竞品概况**（2-3页）：产品基本信息、市场表现、商业模式
- **深度分析**（核心部分，10-15页）：
  - 水面之上：UI/UX分析、功能清单、交互流程
  - 水面之下：模型能力边界、AI人格、交互智能度
  - 内部工作流：反推的工作流（标注置信度）
  - 数据策略：数据飞轮设计、数据壁垒
  - 商业价值：商业模式、用户价值、市场定位
- **多竞品对比**（如适用，5-8页）：对比矩阵、差异化分析、竞争格局
- **结论与建议**（2-3页）：核心发现总结、竞品优劣势分析、对我方的启示和建议、下一步行动建议
- **附录**（可选）：测试用例清单、详细测试数据、参考资料和来源

#### 8.3 报告质量检查清单
- [ ] 是否明确了分析目标和范围？
- [ ] 是否覆盖了技术、商业、用户三个维度？
- [ ] 是否有量化评估和对比？
- [ ] 是否标注了推测的置信度？
- [ ] 是否有验证机制和外部基准？
- [ ] 是否识别并防范了认知偏差？
- [ ] 是否有明确的结论和行动建议？
- [ ] 是否标注了报告有效期和更新计划？

## 何时读取参考资料

- 需要完整的 AI 竞品分析检查清单时 → 读取 `references/checklist.md`
- 需要针对竞品进行深度信息检索的提示词模板时 → 读取 `references/prompts.md`

## 快速诊断模式（极速版）

如果你只需针对特定维度进行分析，可以跳过全量流程。根据分析目标选择对应的诊断方向：

### 选择指南

**选择「诊断能力边界」的场景**：
- 需要评估竞品的技术能力上限
- 需要对比多个竞品的技术水平
- 时间有限但需要快速了解竞品实力
- **执行方法**：直接进行压力测试，评估模型在关键维度的表现（建议测试3-5个核心维度）

**选择「诊断工作流」的场景**：
- 需要了解竞品的技术架构
- 需要判断竞品是否使用了RAG、多Agent等高级技术
- 需要为技术选型提供参考
- **执行方法**：快速测试意图识别、工具调用、RAG检索、后处理四个环节（每个环节设计2-3个测试用例）

**选择「诊断 W 轴（数据飞轮）」的场景**：
- 需要评估竞品的进化潜力
- 需要了解竞品如何收集和利用用户数据
- 需要判断竞品是否具备持续改进能力
- **执行方法**：直接分析竞品的反馈入口与采纳率数据（重点关注显性反馈机制和隐性反馈信号）

**选择「诊断 XYZ 定位」的场景**：
- 需要快速了解竞品的市场定位
- 需要判断竞品的差异化策略
- 需要为产品定位提供参考
- **执行方法**：通过官网首页文案、产品介绍、定价策略快速判断其收益契约（X轴）、垂直化程度（Y轴）、场景化深度（Z轴）

**组合使用**：可以同时选择2-3个诊断方向，形成快速但全面的分析报告。

## 使用示例

### 示例1：快速分析（极速版）

**用户**："帮我快速分析一下 DeepSeek 的竞品定位。"

**执行流程**：
1. **选择诊断方向**：选择"诊断XYZ定位"和"诊断能力边界"（组合使用）
2. **XYZ定位分析**（15分钟）：
   - 访问DeepSeek官网，分析首页文案："极致的成本优势"、"高性能推理"
   - 判断：X轴（收益化）= 成本优势 + 性能优势，Y轴（垂直化）= 技术研发场景，Z轴（场景化）= 代码生成场景
   - 结论：DeepSeek属于"替代-认知"型产品
3. **能力边界测试**（30分钟）：
   - 设计测试用例：代码生成、逻辑推理、多轮对话记忆
   - 执行测试并记录结果：
     - 代码生成：9/10分（测试用例：生成Python爬虫代码 → 代码质量高，逻辑清晰）
     - 逻辑推理：7/10分（测试用例：复杂逻辑问题 → 能处理但偶尔有遗漏）
     - 多轮对话记忆：6/10分（测试用例：10轮对话后回溯 → 能记住前5轮，后续模糊）
4. **快速结论**：DeepSeek在技术研发场景具有明显优势，但在通用对话场景竞争力有限

### 示例2：深度分析（标准版）

**用户**："帮我深度分析一下 DeepSeek 的竞品定位。"

**执行流程**：
1. **阶段0：分析前准备**（10分钟）
   - 明确分析目标：技术团队评估，用于技术选型
   - 时间预算：标准版（1天）
   - 优先级：核心维度（模型能力边界、内部工作流、数据策略）

2. **阶段1：数据采集**（1小时）
   - 使用Kimi深度搜索，使用prompts.md中的"架构深度探测"模板
   - 检索结果：发现DeepSeek基于自研模型，采用单Agent架构
   - 收集公开信息：技术博客、GitHub仓库、用户反馈

3. **阶段2：水面之上分析**（1小时）
   - 战略层：DeepSeek属于"替代-认知"型产品，X轴=成本优势+性能优势
   - 范围层：聚焦代码生成、技术问答场景
   - 结构层：W轴数据闭环通过开源社区反馈形成

4. **阶段3：水面之下分析**（2小时）
   - **能力边界测试**：
     - 设计10个测试用例，覆盖6个测试维度
     - 记录详细测试结果和评分依据
     - 绘制能力雷达图
   - **工作流反推**：
     - 意图识别层测试：输入"写代码"、"解释概念"、"闲聊" → 发现能准确识别代码生成意图
     - 决策规划层测试：测试工具调用能力 → 发现不支持外部工具调用
     - 知识检索层测试：测试RAG能力 → [中置信度]可能使用了代码专用知识库
     - 生成封装层测试：观察输出格式 → 发现代码输出有格式化处理

5. **阶段4：工作流反推**（1小时）
   - 绘制推测的工作流程图
   - 标注置信度：[高]单Agent架构，[中]代码专用RAG，[低]可能有代码优化后处理

6. **阶段5：商业分析**（1小时）
   - 商业模式：开源+商业化混合模式
   - 定价策略：免费开源版 + 付费API服务
   - 市场定位：面向开发者和技术团队

7. **阶段6-8：对比、质量控制、报告**（2小时）
   - 与ChatGPT、Claude对比
   - 认知偏差检查
   - 生成完整分析报告

**输出结果**：生成15页深度分析报告，包含能力雷达图、工作流推测图、对比矩阵等。
