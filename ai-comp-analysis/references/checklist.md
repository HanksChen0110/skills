# AI 产品竞品分析检查清单

本清单基于AI产品冰山分析模型，结合传统用户体验五要素与AI原生产品三维模型（XYZW），用于深度剖析AI竞品。

## 阶段 0：分析前准备

- [ ] **明确分析目标与边界**
  - [ ] 分析目标用户是谁？（产品经理/技术团队/投资人）
  - [ ] 分析深度如何？（快速扫描 vs 深度拆解）
  - [ ] 分析时间范围？（一次性分析 vs 持续跟踪）
- [ ] **确定分析优先级框架**
  - [ ] 核心维度（必须深入）：模型能力边界、内部工作流、数据策略
  - [ ] 重要维度（重点关注）：AI人格、交互智能度、商业价值
  - [ ] 辅助维度（快速扫描）：UI设计、功能清单、市场定位
- [ ] **建立分析假设清单**
  - [ ] 需要验证的核心假设（如："竞品使用了RAG技术"）
  - [ ] 需要证伪的假设（如："竞品只是简单调用API"）
  - [ ] 开放性问题（如："竞品的差异化策略是什么？"）
- [ ] **引入外部基准**
  - [ ] 行业标准测试集（MMLU、HellaSwag等）
  - [ ] 第三方评测数据（学术论文、行业报告）
  - [ ] 历史对比数据（竞品在不同时期的性能变化）

## 阶段 1：水面之上分析（传统5UX框架）

### 1. 战略层 (Strategy) & AI 核心定位
- [ ] **企业愿景与产品定位**：竞品要解决什么核心问题？
- [ ] **XYZ 模型分析**：
  - [ ] **X轴-收益化 (Profitability)**：竞品交付的可感知收益是什么？（如：省时、提质、降本）
  - [ ] **Y轴-垂直化 (Verticality)**：深耕哪个行业？补齐了哪些通用模型无法覆盖的知识或流程？
  - [ ] **Z轴-场景化 (Scenariolization)**：重构了哪个具体场景？人机协作边界如何定义（90% 自动还是辅助）？
- [ ] **技术定位矩阵**：
  - [ ] **赋能 vs 替代**：它是 Sidecar（副驾驶）还是 Agent（数字员工）？
  - [ ] **感知 vs 认知**：核心能力是看听识别还是思考决策？

### 2. 范围层 (Scope) & 场景实现
- [ ] **核心功能集**：对应应用场景维度（X-自动化/Y-决策/Z-能力拓展）
- [ ] **功能架构**：支持哪些任务规划、触发、监控、回滚？
- [ ] **业务流程设计**：AI 在流程中承担什么责任？
- [ ] **防伪节点 (Anti-Hallucination)**：
  - [ ] **幻觉控制**：它如何处理"非确定性"？是否有明确的兜底策略（Fallback）？
  - [ ] **真伪判定**：它是否会一本正经地胡说八道？用户如何校验其生成内容的真伪？

### 3. 结构层 (Structure) & 数据飞轮
- [ ] **W轴-数据闭环**：
  - [ ] **反馈机制**：显性（点赞/点踩/评分）与隐性（采纳率/修改距离）
  - [ ] **进化逻辑**：用户数据如何反哺模型？是静态应用还是进化应用？
- [ ] **信息架构与用户旅程**：用户如何从意图输入到达结果产出？

### 4. 框架层 (Skeleton) & 交互设计
- [ ] **交互框架**：是对话式、画布式还是嵌入式（Sidecar）？
- [ ] **人机协作节点**：在哪里需要人类干预？在哪里由 AI 主导？
- [ ] **标签与导航设计**：AI 生成内容的组织方式

### 5. 表现层 (Surface) & AIGC 质量
- [ ] **视觉与动效**：AI 生成过程的透明度（思考过程展示、加载动效）
- [ ] **产出质量**：生成内容的风格一致性、准确度、审美水平
- [ ] **框架效应**：结果呈现的引导性（如：90% 成功率 vs 10% 失败率）

### 水面-水下连接点分析
- [ ] 为什么选择这种交互方式？（背后反映的技术决策）
- [ ] 为什么是这个入口/触发时机？（反映的产品策略）
- [ ] UI设计选择背后的技术约束（如：为什么限制输入长度？可能反映模型上下文窗口限制）

## 阶段 2：水面之下分析（核心能力拆解）

### 2.1 模型能力边界与压力测试
- [ ] **逻辑推理能力**：逻辑悖论问题、复杂推理链（评分：___/10分）
- [ ] **知识边界与时效性**：上周发生的具体新闻、最新数据（评分：___/10分）
- [ ] **多轮上下文记忆能力**：连续对话10轮后的回溯提问（评分：___/10分）
- [ ] **安全与伦理护栏**：诱导性、边缘性问题（评分：___/10分）
- [ ] **抗干扰与纠错能力**：故意输入错别字或模糊指令（评分：___/10分）
- [ ] **边缘场景测试**：
  - [ ] 多语言混合输入（中英文混杂、特殊字符）
  - [ ] 极端上下文长度（超长输入、超短输入）
  - [ ] 不同环境下的表现差异（移动端vs桌面端、不同网络环境）
- [ ] **能力边界可视化**：绘制"能力雷达图"，直观展示竞品在各维度的表现
- [ ] **对比基准**：与行业平均水平、头部产品进行对比

### 2.2 交互智能度评估
- [ ] **澄清与追问能力**：面对模糊指令时，是直接猜测还是会主动追问澄清，二次确认
- [ ] **优雅的失败案例（Graceful Failure）**：当遇到无法解决的问题时，是生硬拒绝还是提供建设性方案？

### 2.3 AI Persona（AI人格/人设）定义
- [ ] **角色定位**：专家、伙伴、仆人，还是创意家？
- [ ] **语言风格**：措辞、语调、网感词汇、Emoji的使用
- [ ] **交互细节**：响应速度、"思考中"的状态呈现等

### 2.4 数据策略（Data Strategy）推断
- [ ] **用户反馈机制**：点赞/点踩、反馈按钮
- [ ] **数据激励体系**：积分、徽章、更高权限等
- [ ] **核心数据壁垒**：是UGC内容，还是私有化数据？

## 阶段 3：内部工作流反推（动态分析流程）

### 3.1 意图识别层（Intent Recognition）
- [ ] 判断产品如何对用户的输入进行分类和定性
- [ ] 测试方法：分别输入"闲聊型"、"问答型"、"指令型"的指令，观察产品的初步反应和处理路径有何不同
- [ ] 推测置信度：[高/中/低]

### 3.2 决策与规划层（Decision & Planning）
- [ ] 判断产品何时、以及如何决定调用外部"工具"（Tools）
- [ ] 测试方法：提出必须依赖外部信息（如实时天气、最新股价）或特定计算（如复杂数学题）才能解决的问题，观察其是否能成功调用相应工具
- [ ] 推测置信度：[高/中/低]

### 3.3 知识检索层（Knowledge Retrieval/RAG）
- [ ] 判断产品是否应用了RAG（检索增强生成）技术来调用私有知识库
- [ ] 测试方法：提出只有该产品特定领域（如某款游戏的攻略、某公司的财报）才可能知道的精准问题。观察其回答是否超出通用大模型的知识范围，以及是否提供"来源"或"引用链接"
- [ ] 推测置信度：[高/中/低]

### 3.4 生成与封装层（Generation & Packaging）
- [ ] 判断产品的最终输出是否经过了后处理
- [ ] 测试方法：观察最终内容的格式、排版、是否有安全审查机制（例如对敏感词的替换）、是否固定的免责声明等
- [ ] 推测置信度：[高/中/低]

### 3.5 反证测试
- [ ] 主动寻找不符合预期的证据（如：如果竞品表现不符合推测，如何解释？）
- [ ] 设置"竞品失败案例"分析维度（哪些场景下竞品表现差？为什么？）
- [ ] 考虑"竞品可能故意隐藏的能力"（某些能力可能被限制而非不存在）

## 阶段 4：商业与用户视角分析

### 4.1 商业模式分析
- [ ] **盈利模式**：如何盈利？（订阅制、按量付费、免费+增值服务）
- [ ] **定价策略**：价格定位如何？与竞品对比
- [ ] **市场定位**：目标用户是谁？市场细分策略
- [ ] **竞争策略**：差异化策略是什么？护城河在哪里？

### 4.2 用户价值感知分析
- [ ] **用户真正关心什么**：不是技术，而是体验和结果
- [ ] **用户痛点**：竞品解决了用户的哪些核心痛点？
- [ ] **用户旅程**：用户如何使用竞品？关键触点在哪里？
- [ ] **价值主张**：竞品向用户传达的核心价值是什么？

### 4.3 市场定位与竞争格局
- [ ] **目标用户画像**：竞品瞄准哪类用户？
- [ ] **市场位置**：在市场上的相对位置（领导者、挑战者、跟随者）
- [ ] **竞争优势**：相比其他竞品的核心优势
- [ ] **竞争劣势**：可能存在的薄弱环节

## 阶段 5：多竞品横向对比（如适用）

### 5.1 对比矩阵设计
- [ ] 选择3-5个核心竞品进行对比
- [ ] 建立统一的评估维度（模型能力、交互智能度、AI人格、数据策略、商业价值等）
- [ ] 每个维度进行量化评分（1-10分）
- [ ] 绘制对比雷达图，直观展示各竞品的优劣势

### 5.2 差异化分析
- [ ] **核心差异**：各竞品之间的核心差异是什么？
- [ ] **差异化策略**：每个竞品选择了什么差异化路径？
- [ ] **差异化效果**：差异化是否成功？用户是否买单？

### 5.3 竞争格局分析
- [ ] **市场分层**：竞品在市场上的相对位置
- [ ] **竞争态势**：是直接竞争还是差异化竞争？
- [ ] **趋势判断**：哪些竞品在上升？哪些在下降？为什么？

## 阶段 6：认知偏差检查与质量控制

### 6.1 分析过程中的认知偏差防范
- [ ] **锚定效应检查**：是否被"冰山模型"框架过度锚定？是否忽略了水面之上的价值？
- [ ] **可得性偏差检查**：是否只考虑了容易想到的测试用例？是否忽略了罕见但关键的边缘场景？
- [ ] **确认偏误检查**：是否只寻找支持"冰山模型"的证据？是否忽略了反例和不符合预期的表现？
- [ ] **过度自信检查**：是否高估了通过测试就能完全理解竞品的能力？是否忽略了推测的不确定性？

### 6.2 分析质量保障机制
- [ ] **验证机制**：
  - [ ] 推测验证方法：通过公开信息验证（技术博客、论文、招聘信息）
  - [ ] 通过多次测试验证（重复测试，观察一致性）
  - [ ] 通过第三方信息交叉验证（行业报告、用户反馈）
  - [ ] 外部基准对比：引入行业标准测试集和第三方评测数据
  - [ ] 专家评审：邀请技术专家或产品专家评审分析报告
- [ ] **更新机制**：
  - [ ] 分析报告有效期：明确标注分析报告的有效期（建议3-6个月）
  - [ ] 定期更新机制：设置定期更新计划（月度/季度）
  - [ ] 触发更新条件：竞品发布重大更新、发现分析报告中的错误、市场环境发生重大变化

## 阶段 7：综合评估

### 7.1 SWOT 分析
- [ ] **优势 (Strengths)**：技术壁垒、数据积淀、特定场景深度
- [ ] **劣势 (Weaknesses)**：响应时延、成本控制、badcase 覆盖
- [ ] **机会 (Opportunities)**：未被满足的垂直需求、长尾场景
- [ ] **威胁 (Threats)**：大厂通用模型降维打击、政策合规性

### 7.2 报告质量检查
- [ ] 是否明确了分析目标和范围？
- [ ] 是否覆盖了技术、商业、用户三个维度？
- [ ] 是否有量化评估和对比？
- [ ] 是否标注了推测的置信度？
- [ ] 是否有验证机制和外部基准？
- [ ] 是否识别并防范了认知偏差？
- [ ] 是否有明确的结论和行动建议？
- [ ] 是否标注了报告有效期和更新计划？
