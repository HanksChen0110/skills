---
date: 2025-12-03
---

# 鲁棒性提示词构建方法论

## 理论篇

### 通用原则

1. **RTF结构化表达**：通过RTF（Role-Task-Format，即角色-任务-格式）结构化地表达，包括风格、语气、长度等。

2. **信息全面清晰**：信息尽量全面、清晰，否则遗漏的部分模型会联想补全，导致输出的幻觉。上下文越长，幻觉越严重。

3. **正面表述**：指导模型"做什么"通常比告诉它"不做什么"更为有效。比如，如果只想要数学问题的最终答案，应当明确指示"只输出最终答案"，而不是说"不要包含解释"。

4. **善用Markdown**：使用标题、加粗** **等语法。使用分隔符区分指令、上下文、示例等。如Claude的XML标签`<instruction>`，`<context>`，`<example>`，或OpenAI的`###`、`""`。

5. **Few-shot增强**：通过few-shot增强模型的理解力和输出性能。

### 静态示例（Few-Shot）

#### 示例质量要求

- **示例质量**：标签缺失，单词打错等都是不好的。
- **示例标签的顺序**：应该是随机间隔分布为佳，避免前面一半都是正样本，后面一半都是负样本。
- **示例标签占比**：正标签示例和负标签示例的个数应该差不多大。
- **示例标签正确性**：示例标签的正确性需要保证。
- **示例格式**：示例格式（在原论文中给出的例子是标点符号，以text: text===label），原论文作者给出的错误示例是text===label。
- **示例相似性**：如果我们的任务是让LLM匹配个人求职意愿与岗位是否匹配，而输入prompt的示例却是关于诸运动员成绩与运动员等级是否匹配的内容，这样肯定是不好的，容易误导LLM。

#### 静态示例最佳实践（英文说明）

1. **Exemplar Quantity**：Include as many exemplars as possible*
2. **Exemplar Ordering**：Randomly order exemplars*
3. **Exemplar Label Distribution**：Provide a balanced label distribution*
4. **Exemplar Label Quality**：Ensure exemplars are labeled correctly*
5. **Exemplar Format**：Choose a common format*
6. **Exemplars Similarity**：Select similar exemplars to the test instance*

### 动态示例

让示例随对话状态变化而动态更新，而非固定预设。

- **上下文感知**：实时捕捉用户反馈、环境数据、对话历史等变量。
- **示例动态生成**：通过算法或规则实时生成适配当前场景的示例。
- **多轮迭代优化**：根据模型输出结果自动修正示例。

### 奥卡姆剃刀原理

在满足以上前提下，遵循**奥卡姆剃刀原理（简单有效）**，即使用最少的词元（token）。

## 进阶技术

### 1. 上下文工程（Context Engineering）

明确任务的背景、目标受众、相关约束条件以及特定领域知识，从而减少幻觉。

- **静态**：写进提示词
- **动态**：RAG / 联网搜索

### 2. 思维树（Tree of Thoughts, ToT）

与思维链的线性不同，思维树适用于需要探索多个可能性，需要先发散再收敛的场景。

**示例**：
"我需要为我的新咖啡馆想一个独特的营销方案，并且预算有限。请你扮演三位不同的营销专家。第一位专家专注于社交媒体，第二位专注于本地社区活动，第三位专注于数字广告。请你们各自提出一个初步的核心想法。然后，互相评估对方想法的优缺点。最后，综合所有讨论，给出一个融合了最佳元素的最终营销方案，并解释为什么这个方案最好。"

**通用提示方法**：
"解决这个问题：[你的复杂问题]。请你生成三种不同的解决思路。对于每种思路，请评估其潜在的优点和缺点。然后，选择你认为最可行的思路，并详细说明执行步骤。"

### 3. 提示链（Prompt Chaining）

当任务过于复杂，很难通过单次提示获得满意的结果时，则可以使用多步骤提示（Multi-Step Prompting），拆分为一系列更简单、顺序执行的子提示，每个子提示专注于任务一个方面/阶段，1的输出结果，直接作为2调用的输入（或输入的一部分），多用于Agents领域。

**示例**：
我们需要从一篇长文档中提取关键信息，然后对这些信息进行总结，接着基于总结进行分析，最后分析结果特定格式输出。试图用一个提示完成所有这些步骤，很容易失败或者导致低质量输出。

**解决方案**：
可以将此分解为：LLM调用1负责提取，其输出作为LLM调用2的输入进行总结，以此类推。而目前很多具备深度思考功能的模型，已经一定程度融合了提示链，通过单次提示也能获得较好的输出结果，但在面对更加复杂的问题，或需要MoE模型的场景下，显式提示链仍然有用。

### 4. 自我提问（Self-Ask）

在回答复杂或模糊的查询之前，先通过自己提出一系列子问题并逐一回答来进行分解和澄清。这有助于模型系统地探索问题的各个方面，确保所有相关信息都得到考虑，从而得出全面的答案。

**示例提示词**：
"我知道我提出的问题很模糊，请你先向自己提问：为了确保所有的相关信息都得到考虑，得到更全面的答案，可以将我的问题拆分成哪些子问题？需要我补充哪些信息？等待我的回复，在所有子问题得到回答，相关信息也得到补充后，再回答我一开始提出的问题。"

### 5. 递归式自我改进（Recursive Self-Improvement, RSI）

让模型扮演自我批评和改进的角色，被广泛应用在Agents领域。

**工作流程**：
1. 模型生成初始回答
2. 模型对自己的回答进行批评和评估
3. 基于批评生成改进版本
4. 重复步骤2-3直到满足质量标准

## 实践建议

### 构建流程

1. **基础构建**：使用RTF框架，确保信息全面清晰
2. **正面表述**：优先使用正面指令，避免负面表述
3. **Few-shot设计**：根据静态示例最佳实践设计示例
4. **Markdown优化**：使用标题、加粗、分隔符等增强可读性
5. **简洁优化**：遵循奥卡姆剃刀原理，使用最少的token

### 进阶应用

1. **上下文工程**：根据任务复杂度选择静态或动态上下文
2. **思维树**：对于需要探索多个可能性的复杂问题使用ToT
3. **提示链**：对于超复杂任务，拆分为多个子提示
4. **自我提问**：对于模糊问题，先进行自我提问澄清
5. **递归改进**：让模型进行自我批评和改进

### 质量检查清单

- [ ] 是否使用RTF框架结构化表达？
- [ ] 信息是否全面清晰，避免遗漏？
- [ ] 是否使用正面表述而非负面表述？
- [ ] 是否善用Markdown语法增强可读性？
- [ ] Few-shot示例是否符合质量要求（逼真、统一、相关、均衡、正确）？
- [ ] 是否遵循奥卡姆剃刀原理，使用最少的token？
- [ ] 是否需要应用进阶技术（上下文工程/思维树/提示链/自我提问/递归改进）？

