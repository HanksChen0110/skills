---
date: 2025-11-27
tags:
  - AI
  - Agent
  - 智能体
  - 工作流
---

# Agent搭建方法论

> 来源：WaytoAGI COZE训练营活动感想与总结

## 一、Agent的四要素

### 1. 思考：大模型
Agent的核心能力来源，什么环节用什么LLM，需要综合考虑节能、输出效率、输出效果等多方面因素。
- 以及怎么激活大模型的能力（prompt），提示词工程

### 2. 规划：反射、自我反思、思维链、子目标拆解
- 工作流

### 3. 记忆：长期/短期记忆，上下文记忆，知识库

### 4. 执行：调用工具、使用技能

## 二、智能体构建的六大核心要素

1. **大模型**：如何去选择合适的大模型来生成适合这个环节的内容，如何对大模型进行合适的设置
2. **提示词工程**：如何撰写和使用优秀的提示词发挥大模型的能力
3. **工作流**：如何用工作流将业务完整和准确的复现
4. **技能/工具**：如何选择合适的技能和正确的工具
5. **知识库**：如何构建更健壮和准确的知识库，提升回答准确度
6. **上下文记忆**：如何配置上下文记忆，让模型能够响应个性需求

## 三、工作流搭建

### 工作流的心法口诀

从简单到复杂，
从动脑到行动，
从单点到流程，
从随机到可控，
从手动到自动。

### 工作流搭建的学习方法

成年人学习很多事情都是先从实践中学习，从0到1先做一坨垃圾出来，然后再去慢慢优化，解决问题的过程本身就是学习，解决掉一个问题就会成长一些，与目标中间的模糊就更少一些。

但是光是只会搭一些60分的作品出来肯定不够，前面做东西可以先实践再学，但最终还是需要再返回去学习理论知识，这样才能充分理解并创造真正有价值的东西。

## 四、AI架构沙盘

在思考一个AI如何架构时，可采用以下维度：

### 基础设施：算力来源/算力大小/算力能耗

- 算力来源决定部署方式，本地还是云端，云端部署的费用结构就是Token制，本地部署的费用结构就是显卡算力集群和能耗
- 算力大小决定并发，计算的响应速度等
- 算力能耗主要体现在本地集中部署的时候，规模化时会对能耗进行规划对经济效益有很大影响，比如采用绿电、液冷等

### 模型：模型能力-模型选型/本地（微调要怎么做）or在线

根据不同的需求和场景，选择合适的模型。**核心思想：选择合适的大模型最重要的就是验证AI擅不擅长做你想他做的事**

- 比如需要长文本时，选择Kimi K2模型
- 需要深度思考有质量的输出时，选择现在深度思考更强的Deepseek或者豆包1.6等
- 需要文生图时，选择Nano Banana Pro或者MJ或者Seedream4等
- 需要视频制作时，用Sora或者即梦等

**本地化与微调**：
- 如果对本地化有要求，可以用ollama把模型拉到本地来使用
- 可以用火山方舟/阿里云百炼/百度千帆等工具对模型进行微调，小规模训练，使模型在某些指定领域拥有更快的响应速度

### 数据：数据来源/数据清洗/增强

**数据来源**：可以是实时网络搜索/企业自有的数据库/飞书云文档/各种表格/图片等

**数据清洗**：可以用阿里云百炼进行测试验证

**变量**：用来存储动态变化的数据，使Agent可以根据不同的情况进行灵活调整，常见变量包括string字符串/object对象/Array数组/integer整数/float浮点数/boolean布尔值等

### 提示词：prompt工程-激发LLM能力（提示词的结构模板）

**内容表达**：话在嘴边说不出来，我的知识/认知盲区。这两者都只有自己去多练习多感悟才能从中获得经验与体会
- 表达力的本质是用最准确的语言调动对方的===理解力===和===执行力===

**形式技巧**：
- 一阶-三板斧：角色/任务/输出
- 二阶-魔法棒：用魔法打败魔法
- 三阶-结构化：追求复用和稳定
- 四阶-分治法：分而治之单点击破

**创作过程**：解构-建构-压缩-表达

### 业务流程：Agent/Workflow/MCP接入

### 使用方式：原生产品/API接入/MCP被接入/Iframe/SDK

## 五、RAG（检索增强生成）详解

### RAG的关键概念

- 引用数量
- 关键词检索
- 向量检索
- 混合检索
- 混合相似度阈值
- 关键词相似度权重

### RAG的过程

**分块策略**

**Embedding文本转向量**：
- 有专门的Embedding模型
- 输入文本→分词→Embedding模型处理→生成向量→存到向量数据库→查询与检索

**检索过程**：根据向量数据库中对应数据的权重分布，匹配对应的数据

**搜索类型**：
- 语义检索
- 全文检索
- 混合检索（全文/语义检索）

**搜索阶段**：
- 粗排阶段：剔除不符合条件的文本，返回第一阶段检索结果
- 精排阶段：根据粗排结果做相似度计算，得出符合条件的文本结果

**召回策略**：
- 自动调用：调用知识库并使用召回的内容辅助生成回复
- 按需调用：根据提示词自行判断是否调用，并使用召回内容辅助生成

**查询改写**：联系上下文来生成回复

**ReRank结果重排**：根据用户的输入将得出的文本按照权重从高到低展示给用户

### RAG调优

- 当待检索文本中包含大量相似性文本时：可适当提高【混合相似度阈值】，以检索出相似度更高的文本段
- 当用户希望使用关键词过滤文本时，可使用【关键词检索】；希望用语义去检索文本时，可使用【向量检索】
- 当用户的问题中与文档更多的是关键词联系，关键词少且精：可以提高【关键词相似度权重】，或者尝试使用【关键词检索】
- 当用户的问题和文档内容很明显没有关键词联系，但是有意思相近的部分，尽量使用【向量检索】，或者降低【关键词相似度权重】
- 当知识库设置的语言是非英语的外语时，默认使用【向量检索】且不可更改

### RAG召回率

**计算方法**：用query和Ground truth组合成测试集，针对每个query比对检索结果与测试集中的标准答案，统计命中文档数量，该数量除以知识库中相关文档总数即为召回率。不过召回率还应该和精准率结合使用，不能只是召回一些不相关的，还需要引入类似F1分数等综合指标进行全面评估。

> RAG 中的 F1 分数是精确率（Precision）与召回率（Recall）的调和平均数，用于综合评估检索模块的 "准" 与 "全"，取值 0–1，1 为最优。

**召回率优化方法**：最重要是改进检索模型和优化知识库结构。

### 传统RAG vs Agentic RAG

- **传统RAG**：就像图书管理员，你要什么就去知识库里找到然后给你（搜一下然后补进这次的生成里面）
- **Agentic RAG**：会按照Agent的一系列操作（思考、规划、记忆、工具）等来辅助这一次的搜索尽可能符合用户的需求，这里面就还涉及到了意图识别、对抗校准等一系列复杂设定
	- eg：豆包是典型的Agentic RAG的使用案例：在其思考过程里面会体现"分析为什么用户要问这个东西"，"看到的资料和用户所提问的内容的关联"，"可以调用哪些领域的哪些工具来提高检索的准确度"

## 六、提示词Prompt工程

### 什么是prompt工程

prompt工程是专注于用自然语言开发、设计和优化提示，以增加LLM的输出，提供一种引导模型的行为从而达到实现用户期望的方法。

### 有什么好处

- 可以提高模型的能力，增强安全性和稳定性
- 可以使用领域知识和外部工具增强模型，无需更改模型参数或进行微调
- 通过更高质量的输入获得更高质量的输出

### 提示的要素

- **指令**：LLM需要执行的任务
- **上下文**：用于引导模型的外部信息
- **输入数据**：需要响应的输入
- **输出指示**：输出类型或格式

### 最佳实践要素

- 简明扼要：自然流畅，语句连贯，避免孤立的关键字和短语
- 必要时提供上下文
- 针对适当的响应类型使用相应的指令：对输出结果的类型进行约束
- 在提示中考虑输出：使模型专注于输出适当的内容
- 以疑问句开始提示
- 提供示例响应
- 分解复杂的任务
- 反复尝试并发挥创意

### 如何选择合适的prompt策略

- **嵌入式搜索RAG**：需要特定的、最新的或非公开的知识。基于嵌入的搜索，RAG；
- **零样本**：不需要特定的、最新的或非公开的知识。任务简单明了，不需要多个例子来来说明。使用单一、清晰的提示，不提供示例，直接要求LLLM做出回应；
- **少样本**：任务复杂，需要多个例子来来说明但不需要专门的技能、风格或模式，采用少样本提示的方式，在提示词中为LLM提供几个例子引导其回答
- **微调**：任务复杂，需要多个例子来来说明；需要专门的技能、风格或模式，则需要进行微调，在特定领域、技能或风格的训练集上微调LLM

在提示词写法上可以遵循[[12则提示词框架]]和[[结构化提示词的方法论]]，采用定量或定性的方式进行评估，或用阿里云百炼、火山方舟等模型管理工具优化提示词

### 提示词评估实施方法

#### 定量评估

判断 Prompt 优化后的质量是否提高，可以通过以下几个方面进行评估。

**自动化指标**：
- BLEU（Bilingual Evaluation Understudy）：用于评估生成文本与参考文本的 n-gram 重叠程度。较高的 BLEU 分数表示生成文本与参考文本更相似。
- ROUGE（Recall-Oriented Understudy for Gisting Evaluation）：用于评估生成文本与参考文本的重叠程度，特别适用于摘要任务。
- METEOR：考虑了词形变化和同义词，提供更细粒度的评估。

**统计分析**：
- 准确率：计算生成内容中正确信息的比例。
- 召回率：计算生成内容中覆盖参考信息的比例。

#### 定性评估

**人工评估**：
- 相关性：评估生成内容与 Prompt 的相关性，确保生成内容准确回答或响应了 Prompt。
- 流畅性：检查生成内容的语法和语言流畅性。
- 创新性：评估生成内容的创造性和新颖性。

**用户反馈**：
- 满意度调查：收集用户对生成内容的满意度评分。
- 用户评论：分析用户对生成内容的具体反馈和建议。

#### 实验对比

**A/B 测试**：
- 实验设计：将用户随机分为两组，一组使用优化前的 Prompt，另一组使用优化后的 Prompt。
- 结果分析：比较两组用户的行为和反馈，判断优化效果。

**对照实验**：
- 基准测试：使用相同的测试集对优化前后的模型进行评估，比较性能指标。

#### 实施步骤

1. 收集数据：收集优化前后的 Prompt 和生成结果。
2. 选择评估方法：根据具体需求选择合适的定量和定性评估方法。
3. 执行评估：使用选定的方法对生成内容进行评估。
4. 分析结果：分析评估结果，判断优化效果。
5. 反馈调整：根据评估结果和用户反馈，进一步调整和优化 Prompt。

通过这些方法，可以系统地评估 Prompt 优化后的质量，确保生成内容的相关性、准确性和用户满意度得到提升。

## 七、Agent Builder平台

### 平台发展

**字节扣子**：
- 2024.4：扣子国内版上线
- 2024.7：扣子全面开源
- 2025.4：扣子空间上线

**百度文心**：
- 2023.9：发布平台
- 2024：改了两次名和搞了个计划
- 2025：总是慢一步

> LangGraph是LangChain的低级编排框架，核心定位是图结构工作流编排工具，可以单独使用，但是有这个东西可以让LangChain更好用

### 常见平台对比

| 类型                 | 字节   | 百度           | 阿里    | 美国                       | 德国  | 其他      |
| ------------------ | ---- | ------------ | ----- | ------------------------ | --- | ------- |
| 常见的Agent Builder平台 | COZE | 百度AppBuilder | 阿里云百炼 | LangChain/Amazon Bedrock | n8n | Dify    |
| 常见的Agent工作平台       | 扣子空间 | 秒哒           | Qoder |                          |     | Manus   |
| 常见的IDE平台           | Trae | 文心快码         | Qoder | Cursor                   |     | Coplite |
| 模型管理               | 火山方舟 | 千帆           | 百炼    | Amazon Bedrock           |     |         |


## 关联文档
[[WaytoAGI COZE训练营活动感想与总结]]





































